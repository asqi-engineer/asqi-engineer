systems_under_test:
  openai_gpt4o_mini:
    type: "llm_api"
    params:
      base_url: "http://localhost:4000/v1"
      model: "gpt-4o-mini" # Uses litellm configured model
      api_key: "sk-1234" # API key configured in litellm proxy

  # Direct OpenAI API access with explicit API key
  direct_openai:
    type: "llm_api"
    params:
      base_url: "https://api.openai.com/v1"
      model: "gpt-4o-mini"
      api_key: "sk-your-openai-key-here"

  # LiteLLM proxy using fallback values from .env file
  my_llm_service:
    type: "llm_api"
    params:
      model: "my-model"
      # No base_url or api_key specified - will use fallbacks from .env

  # This SUT is *not* compatible, for demonstrating validation failure
  my_backend_api:
    type: "rest_api"
    params:
      uri: "https://api.example.com/v1/data"

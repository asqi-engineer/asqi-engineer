# yaml-language-server: $schema=../../src/asqi/schemas/asqi_systems_config.schema.json
systems:
  openai_gpt4o_mini:
    type: "llm_api" # The llm_api type requires an openai compatible endpoint, we recommend using a proxy service like litellm
    params:
      base_url: "http://localhost:4000/v1" # Use litellm on port 4000
      model: "openai/gpt-4o-mini" # Uses litellm configured model
      api_key: "sk-1234" # API key configured in litellm proxy

  openai_gpt4o:
    type: "llm_api"
    params:
      base_url: "http://localhost:4000/v1" # Use litellm on port 4000
      model: "openai/gpt-4o" # Uses litellm configured model
      api_key: "sk-1234" # API key configured in litellm proxy

  nova_lite:
    type: "llm_api"
    params:
      base_url: "http://localhost:4000" # Use litellm on port 4000
      model: "bedrock/arn:aws:bedrock:us-east-1:156772879641:inference-profile/us.amazon.nova-lite-v1:0" # Uses litellm configured model
      api_key: ${API_KEY} # You can also use string interpolation like this!

  # Direct OpenAI API access with explicit API key
  direct_openai:
    type: "llm_api"
    params:
      base_url: "https://api.openai.com/v1"
      model: "gpt-4o-mini"
      api_key: ${OPENAI_API_KEY}

  my_llm_service:
    type: "llm_api"
    params:
      model: "my-model"
      env_file: .env # Pass in all env variables from file

  # This SUT is *not* compatible, for demonstrating validation failure
  my_backend_api:
    type: "rest_api"
    params:
      uri: "https://api.example.com/v1/data"

# yaml-language-server: $schema=../../src/asqi/schemas/asqi_score_card.schema.json
#
# Example Score Card demonstrating metric expression capabilities
#
# This score card shows how to use mathematical expressions to combine
# multiple metrics for more sophisticated assessments.
#
# Expression Format:
#   For simple metric paths (backward compatible):
#     metric: "path.to.metric"
#
#   For expressions combining multiple metrics:
#     metric:
#       expression: "metric1 * 0.7 + metric2 * 0.3"
#       values:
#         metric1: "path.to.metric1"
#         metric2: "path.to.metric2"
#
#   The 'values' dict maps expression variable names to metric paths.
#   Variable names can be anything valid in Python expressions.
#   This allows clean expressions while extracting from complex nested paths.
#

score_card_name: "Metric Expression Examples"

indicators:
  # Example 1: Simple weighted average of two metrics
  - id: "weighted_quality_score"
    name: "Weighted Quality Score (70% accuracy, 30% relevance)"
    apply_to:
      test_id: "chatbot_sim_test"
    metric: 
      expression: "0.7 * accuracy + 0.3 * relevance"
      values:
        accuracy: "average_answer_accuracy"
        relevance: "average_answer_relevance"
    assessment:
      - outcome: "Excellent"
        condition: "greater_equal"
        threshold: 0.9
        description: "Outstanding performance across metrics"
      - outcome: "Good"
        condition: "greater_equal"
        threshold: 0.75
        description: "Good performance with room for improvement"
      - outcome: "Needs Improvement"
        condition: "less_than"
        threshold: 0.75
        description: "Performance below acceptable threshold"

  # Example 2: Using min() to enforce all metrics meet threshold
  - id: "minimum_acceptable_score"
    name: "All metrics must meet minimum threshold"
    apply_to:
      test_id: "chatbot_sim_test"
    metric:
      expression: "min(accuracy, relevance, consistency)"
      values:
        accuracy: "average_answer_accuracy"
        relevance: "average_answer_relevance"
        consistency: "consistency_score"
    assessment:
      - outcome: "Pass"
        condition: "greater_equal"
        threshold: 0.7
        description: "All metrics meet minimum acceptable threshold"
      - outcome: "Fail"
        condition: "less_than"
        threshold: 0.7
        description: "One or more metrics below minimum threshold"

  # Example 3: Using max() to find best performing metric
  - id: "best_performance_indicator"
    name: "Best performing metric across test dimensions"
    apply_to:
      test_id: "multi_dimension_test"
    metric:
      expression: "max(dim1, dim2, dim3)"
      values:
        dim1: "dimension1_score"
        dim2: "dimension2_score"
        dim3: "dimension3_score"
    assessment:
      - outcome: "Strong"
        condition: "greater_equal"
        threshold: 0.85
        description: "At least one dimension shows strong performance"
      - outcome: "Moderate"
        condition: "greater_equal"
        threshold: 0.7
        description: "Best dimension shows moderate performance"
      - outcome: "Weak"
        condition: "less_than"
        threshold: 0.7
        description: "All dimensions show weak performance"

  # Example 4: Using avg() for balanced assessment
  - id: "average_performance"
    name: "Average performance across all test metrics"
    apply_to:
      test_id: "comprehensive_test"
    metric:
      expression: "avg(m1, m2, m3, m4, m5)"
      values:
        m1: "metric1"
        m2: "metric2"
        m3: "metric3"
        m4: "metric4"
        m5: "metric5"
    assessment:
      - outcome: "A"
        condition: "greater_equal"
        threshold: 0.9
      - outcome: "B"
        condition: "greater_equal"
        threshold: 0.8
      - outcome: "C"
        condition: "greater_equal"
        threshold: 0.7
      - outcome: "D"
        condition: "greater_equal"
        threshold: 0.6
      - outcome: "F"
        condition: "less_than"
        threshold: 0.6

  # Example 5: Complex expression with multiple operations
  - id: "composite_score_capped"
    name: "Composite score with maximum cap"
    apply_to:
      test_id: "performance_test"
    metric:
      expression: "min((0.4 * speed + 0.3 * accuracy + 0.3 * reliability), 1.0)"
      values:
        speed: "speed_score"
        accuracy: "accuracy_score"
        reliability: "reliability_score"
    assessment:
      - outcome: "Optimal"
        condition: "greater_equal"
        threshold: 0.95
        description: "Near-perfect composite performance"
      - outcome: "Satisfactory"
        condition: "greater_equal"
        threshold: 0.8
        description: "Satisfactory composite performance"
      - outcome: "Unsatisfactory"
        condition: "less_than"
        threshold: 0.8
        description: "Composite performance needs improvement"

  # Example 6: Arithmetic operations for derived metrics
  - id: "performance_margin"
    name: "Performance margin above baseline"
    apply_to:
      test_id: "comparison_test"
    metric:
      expression: "actual - baseline"
      values:
        actual: "actual_performance"
        baseline: "baseline_performance"
    assessment:
      - outcome: "Significantly Better"
        condition: "greater_equal"
        threshold: 0.2
        description: "Performance exceeds baseline by 20%+"
      - outcome: "Moderately Better"
        condition: "greater_equal"
        threshold: 0.1
        description: "Performance exceeds baseline by 10-20%"
      - outcome: "Comparable"
        condition: "greater_than"
        threshold: -0.1
        description: "Performance within 10% of baseline"
      - outcome: "Worse"
        condition: "less_equal"
        threshold: -0.1
        description: "Performance below baseline"

  # Example 7: Backward compatible simple metric path
  - id: "simple_pass_rate"
    name: "Simple pass rate check (no expression)"
    apply_to:
      test_id: "basic_test"
    metric: "overall_pass_rate"
    assessment:
      - outcome: "Pass"
        condition: "greater_equal"
        threshold: 0.8
      - outcome: "Fail"
        condition: "less_than"
        threshold: 0.8

  # Example 8: Nested metric paths with clean expression variables
  - id: "nested_metrics_example"
    name: "Using nested metrics in expressions"
    apply_to:
      test_id: "structured_results_test"
    metric:
      expression: "pass_rate + accuracy"
      values:
        pass_rate: "results.summary.pass_rate"
        accuracy: "results.summary.accuracy"
    assessment:
      - outcome: "Excellent"
        condition: "greater_equal"
        threshold: 1.8
        description: "Combined nested metrics show excellent performance"
      - outcome: "Good"
        condition: "greater_equal"
        threshold: 1.5
      - outcome: "Poor"
        condition: "less_than"
        threshold: 1.5

name: "inspect_evals"
version: "1.0"
description: "Run an Inspect Evals task (e.g. gsm8k, mmlu_0_shot, truthfulqa). Provide: evaluation (task name), optional evaluation_params (task args), and limit (sample cap). See https://ukgovernmentbeis.github.io/inspect_evals/ for task docs."
host_access: true

input_systems:
  - name: "system_under_test"
    type: "llm_api"
    required: true
    description: "The LLM system being tested"

input_schema:
  - name: "evaluation"
    type: "string"
    required: true
    description: |
      Name of the Inspect Evals task to run. Categories (enabled tasks only):
        Cybersecurity: cyse3_visual_prompt_injection, threecb, cybermetric_* (80/500/2000/10000), cyse2_*, sevenllm_*, sec_qa_*, gdm_intercode_ctf
        Safeguards: abstention_bench, agentdojo, agentharm (+_benign), lab_bench_*, mask, make_me_pay, stereoset, strong_reject, wmdp_*
        Mathematics: aime2024, gsm8k, math, mgsm, mathvista
        Reasoning: arc_challenge, arc_easy, bbh, bbeh (+_mini), boolq, drop, hellaswag, ifeval, lingoly (+_too), mmmu_*, musr, niah, paws, piqa, race_h, winogrande, worldsense, infinite_bench_*
        Knowledge: agie_*, air_bench, chembench, commonsense_qa, gpqa_diamond, healthbench, hle, livebench, mmlu_* , mmlu_pro, medqa, onet_m6, pre_flight, pubmedqa, sosbench, sciknoweval, simpleqa, truthfulqa, xstest
        Scheming: agentic_misalignment, gdm_*
        Multimodal: zerobench, zerobench_subquestions (others currently disabled in this container)
        Bias: bbq, bold
        Personality: personality_BFI, personality_TRAIT
        Writing: writingbench
  - name: "limit"
    type: "integer"
    required: false
    description: "Limit samples to evaluate e.g. 10 or 10-20. Default: 10"
    default: 10
  - name: "evaluation_params"
    type: "object"
    required: false
    description: |
      Task-specific parameter map passed to the underlying @task function.
      Common keys include: fewshot (gsm8k), fewshot_seed (gsm8k), cot (mmlu_*), subjects (mmlu_*, agie_*), language (mmlu_*), use_fast_sampling (abstention_bench), dataset_name (abstention_bench subset), token_limit (some code tasks), seed (many tasks).
      Provide an object, e.g.: {"fewshot": 5} or {"subjects": ["anatomy","astronomy"], "cot": true}.
      Omit or use {} for defaults. See task source or docs site for full parameter context: https://ukgovernmentbeis.github.io/inspect_evals/
    default: {}
  - name: "limit"
    type: "integer"
    required: false
    description: "Maximum number of samples to evaluate. Default: 10"
    default: 10
  - name: "sandbox_params"
    type: "object"
    required: false
    description: |
      Optional resource limits for sandbox containers. Provide an object with:
      - cpus: Number of CPUs (default: 1.0)
      - mem_limit: Memory limit as string e.g. "1g", "512m" (default: "1g")
      Example: {"cpus": 2.0, "mem_limit": "2g"}
    default: {}

output_metrics:
  - name: "success"
    type: "boolean"
    description: "Whether the test execution completed successfully"
  - name: "evaluation"
    type: "string"
    description: "The evaluation that was run"
  - name: "evaluation_params"
    type: "object"
    description: "The parameters used for the evaluation"
  - name: "total_samples"
    type: "integer"
    description: "Total number of samples evaluated"
  - name: "metrics"
    type: "object"
    description: "Additional metrics returned by the evaluation"
  - name: "log_dir"
    type: "string"
    description: "Path to stored evaluation logs (present when output volume is configured)"

environment_variables:
  - name: "HF_TOKEN"
    required: false
    description: "Require access to gated HuggingFace datasets"

from typing import Any, Dict, List, Literal, Optional, Union

from pydantic import BaseModel, Field

# ----------------------------------------------------------------------------
# Schemas for manifest.yaml (Embedded in Test Containers)
# ----------------------------------------------------------------------------


class SUTSupport(BaseModel):
    """Defines a type of SUT the container can test."""

    type: str = Field(..., description="The SUT type, e.g., 'llm_api' or 'rest_api'.")
    required_config: Optional[List[str]] = Field(
        None,
        description="List of config keys required by the entrypoint for this SUT type.",
    )


class InputParameter(BaseModel):
    """Defines a parameter that can be passed to the test container."""

    name: str
    type: Literal["string", "integer", "boolean", "list"]
    required: bool = False
    description: Optional[str] = None


class OutputArtifact(BaseModel):
    """Defines a file artifact generated by the test container."""

    name: str
    path: str
    description: Optional[str] = None


class Manifest(BaseModel):
    """Schema for the manifest.yaml file inside a test container."""

    name: str = Field(..., description="The canonical name for the test framework.")
    version: str
    description: Optional[str] = None
    image_name: str = Field(..., description="The full Docker image name and tag.")
    supported_suts: List[SUTSupport] = Field(
        ..., description="Declares which SUT types this framework can handle."
    )
    input_schema: List[InputParameter] = Field(
        [], description="Defines the schema for the user-provided 'params' object."
    )
    output_metrics: List[str] = Field(
        [], description="Defines expected high-level metrics in the final JSON output."
    )
    output_artifacts: Optional[List[OutputArtifact]] = None


# ----------------------------------------------------------------------------
# Schema for suts.yaml (User-provided)
# ----------------------------------------------------------------------------


class SUTDefinition(BaseModel):
    """A single System Under Test definition."""

    type: str = Field(
        ...,
        description="e.g., 'llm_api', 'rest_api'. Used to match with test container capabilities.",
    )
    config: Dict[str, Any] = Field(
        ...,
        description="Configuration specific to the SUT type (e.g., API endpoint, model name).",
    )


class SUTsConfig(BaseModel):
    """Schema for the top-level SUTs configuration file."""

    systems_under_test: Dict[str, SUTDefinition]


# ----------------------------------------------------------------------------
# Schema for test_suite.yaml (User-provided)
# ----------------------------------------------------------------------------


class TestDefinition(BaseModel):
    """A single test to be executed."""

    name: str = Field(
        ..., description="A unique, human-readable name for this test instance."
    )
    image: str = Field(
        ...,
        description="The Docker image to run for this test, e.g., 'my-registry/garak:latest'.",
    )
    target_suts: List[str] = Field(
        ...,
        description="A list of SUT names (from suts.yaml) to run this test against.",
    )
    tags: Optional[List[str]] = Field(
        None, description="Optional tags for filtering and reporting."
    )
    params: Dict[str, Any] = Field(
        {}, description="Parameters to be passed to the test container's entrypoint."
    )


class SuiteConfig(BaseModel):
    """Schema for the top-level Test Suite configuration file."""

    suite_name: str
    test_suite: List[TestDefinition]


# ----------------------------------------------------------------------------
# Schema for grading policies (User-provided)
# ----------------------------------------------------------------------------


class PolicyFilter(BaseModel):
    """Defines which test results an indicator applies to."""

    test_name: str = Field(
        ...,
        description="Test name to filter by, e.g., 'run_mock_on_compatible_sut_my_llm_service'",
    )


class AssessmentRule(BaseModel):
    """Individual assessment outcome with condition."""

    outcome: str = Field(
        ..., description="Assessment outcome, e.g., 'PASS', 'FAIL', 'A', 'F'"
    )
    condition: Literal[
        "equal_to",
        "greater_than",
        "less_than",
        "greater_equal",
        "less_equal",
        "all_true",
        "any_false",
        "count_equals",
    ] = Field(..., description="Condition to evaluate against the metric value")
    threshold: Optional[Union[int, float]] = Field(
        None, description="Threshold value for comparison conditions"
    )


class PolicyIndicator(BaseModel):
    """Individual policy indicator with filtering and assessment."""

    name: str = Field(..., description="Human-readable name for this policy indicator")
    apply_to: PolicyFilter = Field(
        ...,
        description="Filter criteria for which test results this indicator applies to",
    )
    metric: str = Field(
        ..., description="JSONPath into test_results object, e.g., 'success', 'score'"
    )
    assessment: List[AssessmentRule] = Field(
        ..., description="List of assessment rules to evaluate against the metric"
    )


class GradingPolicy(BaseModel):
    """Complete grading policy configuration."""

    policy_name: str = Field(..., description="Name of the grading policy")
    indicators: List[PolicyIndicator] = Field(
        ..., description="List of policy indicators to evaluate"
    )

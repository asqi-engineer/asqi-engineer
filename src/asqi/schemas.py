from typing import Any, Dict, List, Literal, Optional, Union

from pydantic import BaseModel, Field

# ----------------------------------------------------------------------------
# Schemas for manifest.yaml (Embedded in Test Containers)
# ----------------------------------------------------------------------------


class SUTSupport(BaseModel):
    """Defines a type of SUT the container can test."""

    type: str = Field(..., description="The SUT type, e.g., 'llm_api' or 'rest_api'.")
    required_config: Optional[List[str]] = Field(
        None,
        description="List of config keys required by the entrypoint for this SUT type.",
    )


class InputParameter(BaseModel):
    """Defines a parameter that can be passed to the test container."""

    name: str
    type: Literal["string", "integer", "float", "boolean", "list", "object"]
    required: bool = False
    description: Optional[str] = None


class OutputMetric(BaseModel):
    """Defines a metric that will be present in the test container's output."""

    name: str
    type: Literal["string", "integer", "float", "boolean", "list", "object"]
    description: Optional[str] = None


class OutputArtifact(BaseModel):
    """Defines a file artifact generated by the test container."""

    name: str
    path: str
    description: Optional[str] = None


class Manifest(BaseModel):
    """Schema for the manifest.yaml file inside a test container."""

    name: str = Field(..., description="The canonical name for the test framework.")
    version: str
    description: Optional[str] = None
    supported_suts: List[SUTSupport] = Field(
        ..., description="Declares which SUT types this framework can handle."
    )
    input_schema: List[InputParameter] = Field(
        [], description="Defines the schema for the user-provided 'params' object."
    )
    output_metrics: Union[List[str], List[OutputMetric]] = Field(
        [],
        description="Defines expected high-level metrics in the final JSON output. Can be a simple list of strings or detailed metric definitions.",
    )
    output_artifacts: Optional[List[OutputArtifact]] = None


# ----------------------------------------------------------------------------
# Schema for suts.yaml (User-provided)
# ----------------------------------------------------------------------------


class LLMAPIConfig(BaseModel):
    """Configuration for LLM API SUTs."""

    base_url: str = Field(
        ...,
        description="Base URL for the OpenAI-compatible API (e.g., 'http://localhost:4000/v1', 'https://api.openai.com/v1')",
    )
    model: str = Field(
        ...,
        description="Model name to use with the API",
    )
    env_file: Optional[str] = Field(
        None,
        description="Path to .env file containing environment variables for authentication",
    )
    api_key: Optional[str] = Field(
        None,
        description="Direct API key for authentication (alternative to env_file)",
    )


class SUTDefinition(BaseModel):
    """A single System Under Test definition."""

    type: str = Field(
        ...,
        description="e.g., 'llm_api', 'rest_api'. Used to match with test container capabilities.",
    )
    params: Dict[str, Any] = Field(
        ...,
        description="Parameters specific to the SUT type (e.g., base_url, model name, API key).",
    )


class SUTsConfig(BaseModel):
    """Schema for the top-level SUTs configuration file."""

    systems_under_test: Dict[str, SUTDefinition]


# ----------------------------------------------------------------------------
# Schema for test_suite.yaml (User-provided)
# ----------------------------------------------------------------------------


class TestDefinition(BaseModel):
    """A single test to be executed."""

    name: str = Field(
        ..., description="A unique, human-readable name for this test instance."
    )
    image: str = Field(
        ...,
        description="The Docker image to run for this test, e.g., 'my-registry/garak:latest'.",
    )
    target_suts: List[str] = Field(
        ...,
        description="A list of SUT names (from suts.yaml) to run this test against.",
    )
    tags: Optional[List[str]] = Field(
        None, description="Optional tags for filtering and reporting."
    )
    params: Dict[str, Any] = Field(
        {}, description="Parameters to be passed to the test container's entrypoint."
    )
    volumes: Optional[Dict[str, Any]] = Field(
        {}, description="Optional input/output mounts."
    )


class SuiteConfig(BaseModel):
    """Schema for the top-level Test Suite configuration file."""

    suite_name: str
    test_suite: List[TestDefinition]


# ----------------------------------------------------------------------------
# Schema for Score Card (User-provided)
# ----------------------------------------------------------------------------


class ScoreCardFilter(BaseModel):
    """Defines which test results an indicator applies to."""

    test_name: str = Field(
        ...,
        description="Test name to filter by, e.g., 'run_mock_on_compatible_sut_my_llm_service'",
    )


class AssessmentRule(BaseModel):
    """Individual assessment outcome with condition."""

    outcome: str = Field(
        ..., description="Assessment outcome, e.g., 'PASS', 'FAIL', 'A', 'F'"
    )
    condition: Literal[
        "equal_to",
        "greater_than",
        "less_than",
        "greater_equal",
        "less_equal",
        "all_true",
        "any_false",
        "count_equals",
    ] = Field(..., description="Condition to evaluate against the metric value")
    threshold: Optional[Union[int, float]] = Field(
        None, description="Threshold value for comparison conditions"
    )


class ScoreCardIndicator(BaseModel):
    """Individual score card indicator with filtering and assessment."""

    name: str = Field(
        ..., description="Human-readable name for this score card indicator"
    )
    apply_to: ScoreCardFilter = Field(
        ...,
        description="Filter criteria for which test results this indicator applies to",
    )
    metric: str = Field(
        ..., description="JSONPath into test_results object, e.g., 'success', 'score'"
    )
    assessment: List[AssessmentRule] = Field(
        ..., description="List of assessment rules to evaluate against the metric"
    )


class ScoreCard(BaseModel):
    """Complete grading score card configuration."""

    score_card_name: str = Field(..., description="Name of the grading score card")
    indicators: List[ScoreCardIndicator] = Field(
        ..., description="List of score card indicators to evaluate"
    )
